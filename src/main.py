from mongodb_connector import fetch_user_query, store_evaluation
from qdrant_connector import fetch_relevant_data
from evaluation.accuracy import compute_accuracy
from evaluation.relevance import compute_relevance
from evaluation.hallucination import detect_hallucination
from sklearn.metrics.pairwise import cosine_similarity
from embedding_utils import embed_text
import numpy as np

def compute_cosine_similarity(vec1, vec2):
    if isinstance(vec1, list):
        vec1 = np.array(vec1) 
    if isinstance(vec2, list):
        vec2 = np.array(vec2)
    return float(cosine_similarity([vec1], [vec2][0][0]))

def evaluate_query_response(query_data):
    """
    Evaluate a single user query and AI response and store the evaluation result.
    Args:
        query_data (dict): A dictionary containing the user's query and the AI's response.
    """
    if not query_data:
        print("No query data found.")
        return
    
    user_query = query_data['user_prompt']['text'] # User's original query
    ai_response = query_data['ai_response']['text'] # AI-generated response
    
    #Convert to embeddings
    query_embedding = embed_text(user_query)
    response_embedding = embed_text(ai_response)

    print(f"Type of query_embedding: {type(query_embedding)}")
    print(f"query_embedding: {query_embedding}")
    
    print(f"Type of response_embedding: {type(response_embedding)}")
    print(f"response_embedding: {response_embedding}")

     # Flatten the embeddings (if necessary)
    query_embedding = query_embedding.flatten().tolist()
    response_embedding = response_embedding.flatten().tolist()

    # Fetch relevant context from Qdrant (simulate fetching relevant data based on the user query)
    relevant_data = fetch_relevant_data(query_embedding)
    print(f"Fetched relevant data: {relevant_data}")
    reference_text = relevant_data.get("text", "")

    reference_embedding = embed_text(reference_text)
    accuracy_score = compute_cosine_similarity(response_embedding, reference_embedding)

    
    relevance_results = compute_relevance(ai_response, reference_text)
    hallucination_results = detect_hallucination(ai_response)

    # Combine all evaluation results
    evaluation_results = {
            "query": user_query,
            "response": ai_response,
            "accuracy": accuracy_score,
            "relevance": relevance_results,
            "hallucination": hallucination_results,
            "matched_collection": relevant_data["collection"],
            "reference_text": reference_text,
    }
    
    # Store the evaluation results in the 'evaluation_collection' in MongoDB
    store_evaluation(query_data['_id'], evaluation_results)
    print(f"Evaluation completed for query ID: {query_data['_id']}")




# Example: This will be triggered after a query-response pair is generated by the chatbot.
if __name__ == "__main__":
    # Fetch a new query and response from MongoDB (you may call this after the chatbot generates a response)
    query_data = fetch_user_query()  # Fetch the most recent query (or use your own logic to select queries)

    if query_data:
        evaluate_query_response(query_data)
    else:
        print("No prompt data available.")
