from mongodb_connector import fetch_user_query, store_evaluation
from qdrant_connector import fetch_relevant_data
from evaluation.accuracy import compute_accuracy
from evaluation.relevance import compute_relevance
from evaluation.hallucination import detect_hallucination
from sklearn.metrics.pairwise import cosine_similarity

def evaluate_query_response(query_data):
    """
    Evaluate a single user query and AI response and store the evaluation result.
    Args:
        query_data (dict): A dictionary containing the user's query and the AI's response.
    """
    user_query = query_data['query']  # User's original query
    ai_response = query_data['response']  # AI-generated response
    
    # Fetch relevant context from Qdrant (simulate fetching relevant data based on the user query)
    relevant_data = fetch_relevant_data(user_query)
    
    # Evaluate accuracy, relevance, and hallucination for the AI response
    accuracy_results = compute_accuracy(ai_response, user_query)
    relevance_results = compute_relevance(ai_response, relevant_data)
    hallucination_results = detect_hallucination(ai_response)

    # Combine all evaluation results
    evaluation_results = {
        "query": user_query,
        "response": ai_response,
        "accuracy": accuracy_results,
        "relevance": relevance_results,
        "hallucination": hallucination_results,
    }
    
    # Store the evaluation results in the 'evaluation_collection' in MongoDB
    store_evaluation(query_data['_id'], evaluation_results)
    print(f"Evaluation completed for query ID: {query_data['_id']}")

# Example: This will be triggered after a query-response pair is generated by the chatbot.
if __name__ == "__main__":
    # Fetch a new query and response from MongoDB (you may call this after the chatbot generates a response)
    query_data = fetch_user_query()  # Fetch the most recent query (or use your own logic to select queries)

    # Assume fetch_user_query returns a list of queries (we can process them one by one)
    for query in query_data:
        # Evaluate the current query and its response
        evaluate_query_response(query)
