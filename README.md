**Project Overview**

This project is a machine learning-based evaluation framework designed to assess the quality of AI-generated chatbot responses. It focuses on evaluating aspects such as: 
  - Response relevance
  - Accuracy
  - Hallucination risk
  - Semantic similarity between user queries, AI responses, and source material

The framework was developed as part of a bachelor's thesis project to help systematically validate chatbot outputs in an industrial environment before deployment. It uses text embeddings and structured evaluation metrics to provide quantitative insights into response quality. 

The system receives conversational data, generates embeddings, applies evaluation metrics, and produces results that help identify weaknesses and areas for improvement in the chatbot.

<img width="798" height="476" alt="image" src="https://github.com/user-attachments/assets/314652ea-e08f-41a3-9f88-2f69db804a53" />

